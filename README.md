# CS370Deep_Q-Learning
Treasure Hunt Game Notebook is a project that implements a deep Q-learning algorithm to solve a maze navigation problem. In this project, a pirate agent must find a hidden treasure located at the bottom-right corner of a maze. The maze is represented by a two-dimensional NumPy array where free cells are denoted by 1.0 and blocked cells by 0.0. The pirate is required to start on a free cell, and its objective is to navigate through the maze by taking one of four possible actions: moving left, up, right, or down.

The project is organized into several components. The file TreasureMaze.py contains the environment’s implementation. It manages the maze layout, updates the pirate’s position based on the chosen action, and computes rewards. Positive rewards are given when the pirate reaches the treasure, while negative rewards are applied for invalid moves, revisiting previously explored cells, or hitting obstacles. To prevent endless wandering, the environment also enforces a minimum reward threshold, beyond which the game is considered lost.

Another essential component is the GameExperience.py file, which implements experience replay. This module stores episodes, each being a sequence of states, actions, rewards, and transitions experienced by the agent. The stored episodes are then randomly sampled to train the neural network model, which approximates the Q-values associated with each action. This technique helps stabilize the learning process by breaking the correlation between consecutive experiences.

The main workflow is integrated into a Jupyter Notebook, which includes code for building and training the neural network using Keras. The qtrain function is the key part of the assignment and is designed to train the model through numerous episodes. In each epoch, the agent starts at a random free cell and chooses actions based on a balance between exploration and exploitation. The network’s predictions, combined with a discount factor, are used to update the Q-values, guiding the agent toward optimal behavior.

Visualization functions are provided to display the maze, highlighting visited cells, the pirate’s current location, and the treasure. After training, users can test the model by simulating a complete game starting from a designated position. The project instructions also include submission guidelines, requiring students to save their work with a filename that includes their name and then submit the notebook file for grading. Overall, this project provides a hands-on experience with deep reinforcement learning concepts, including experience replay, policy learning, and neural network-based Q-value approximation.
